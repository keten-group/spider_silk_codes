{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import r2_score\n",
    "import random\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import csv\n",
    "from numpy import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import date\n",
    "import json\n",
    "# mpl.rcParams['figure.dpi'] = 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length 2575\n",
      "max len 1672.0\n",
      "min len 100.0\n"
     ]
    }
   ],
   "source": [
    "path_directory = \"../raw_data/\"\n",
    "count = 0\n",
    "store_seq = np.zeros((6402,))\n",
    "accepted_type = ['MaSp1', 'MaSp2', 'MaSp3', 'MaSp','MiSp', 'Spidroin']\n",
    "# accepted_type = ['MaSp1', 'MaSp2','MiSp']\n",
    "# accepted_type = ['MaSp1']\n",
    "for filename in os.listdir(path_directory):\n",
    "    spidroin_type = filename.split('_')[5]\n",
    "    if spidroin_type in accepted_type:\n",
    "        sequence = np.loadtxt(path_directory+filename, unpack=True, skiprows=1, dtype=str)\n",
    "        joint_sequence = ''.join(sequence)\n",
    "        store_seq[count, ] = len(joint_sequence)\n",
    "        # print(filename, len(joint_sequence))\n",
    "        count += 1\n",
    "max_len = int(max(store_seq[0:count,]))\n",
    "print('total length',count)\n",
    "print('max len', max(store_seq[0:count,]))\n",
    "print('min len', min(store_seq[0:count,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_feature = {}\n",
    "corr_feature = {}\n",
    "for i in accepted_type:\n",
    "    name_feature.update({i:[]})\n",
    "    corr_feature.update({i:[]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(446, 13)\n"
     ]
    }
   ],
   "source": [
    "all_data = pd.read_csv(\"../mechanical_properties.csv\")\n",
    "# names = np.array([all_data[\"family\"], all_data[\"genus\"], all_data[\"species\"]], dtype=str)\n",
    "# names = names.T\n",
    "# print(all_data)\n",
    "# prop = np.array([all_data[\"toughness\"].to_numpy(), all_data[\"toughness_sd\"].to_numpy(),all_data[\"young's_modulus\"].to_numpy(),all_data[\"young's_modulus_sd\"].to_numpy(),all_data[\"tensile_strength\"].to_numpy(), all_data[\"tensile_strength_sd\"].to_numpy()]).T\n",
    "prop = np.array([all_data[['idv_id',\"toughness\",\"toughness_sd\", \"young's_modulus\", \"young's_modulus_sd\", \"tensile_strength\", \"tensile_strength_sd\", \"strain_at_break\", \"strain_at_break_sd\", \"crystallinity\", \"supercontraction\",\"birefringence\" ,\"birefringence_sd\"]].to_numpy()])\n",
    "prop = np.reshape(prop, (prop.shape[1], prop.shape[2]))\n",
    "print(prop.shape)\n",
    "# print(names[:,2])\n",
    "# print(np.shape(np.unique(names[:,2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max spider silk: 221\n"
     ]
    }
   ],
   "source": [
    "# preparing the data\n",
    "path_directory = \"/home/apa2237/spider/raw_data/\"\n",
    "count = 0\n",
    "present = np.zeros((6402,))\n",
    "not_present = np.zeros((6402,))\n",
    "not_p = 0\n",
    "for filename in os.listdir(path_directory):\n",
    "    spidroin_type = filename.split('_')[5]\n",
    "    if spidroin_type in accepted_type:\n",
    "        id = int(filename.split('_')[1])\n",
    "        pos = np.where(prop[:,0]==id)\n",
    "        if pos[0].size > 0:\n",
    "            present[count,] = id\n",
    "            count += 1\n",
    "        else:\n",
    "            not_present[not_p,] = id\n",
    "            # print(filename, id)\n",
    "            not_p += 1\n",
    "all_present = np.unique(present[0:count,])\n",
    "print('Max spider silk:', len(all_present))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_data(present, accepted_type):\n",
    "    path_directory = \"/home/apa2237/spider/raw_data/\"\n",
    "    count = 0\n",
    "    input_sequence = np.zeros((len(present), 27,max_len), dtype=str)\n",
    "    num_files = np.zeros((len(present),))\n",
    "    seq_length = np.zeros((len(present), 27))\n",
    "    for filename in os.listdir(path_directory):\n",
    "        spidroin_type = filename.split('_')[5]\n",
    "        if spidroin_type in accepted_type:    \n",
    "            sequence = np.loadtxt(path_directory+filename, unpack=True, skiprows=1, dtype=str)\n",
    "            joint_sequence = ''.join(sequence)\n",
    "            # print(joint_sequence)\n",
    "            pos = np.where(present == int(filename.split('_')[1]))[0]\n",
    "            if pos.size > 0:\n",
    "                pos = int(pos[0])\n",
    "                pos = int(np.where(present == int(filename.split('_')[1]))[0])\n",
    "                input_sequence[pos,int(num_files[pos,]), 0:int(len(joint_sequence))] = list(joint_sequence)\n",
    "                seq_length[pos, int(num_files[pos,])] = len(joint_sequence)\n",
    "                num_files[pos, ] += 1\n",
    "    \n",
    "    return input_sequence, num_files, seq_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique amino acids are 21\n"
     ]
    }
   ],
   "source": [
    "# # different amino acids\n",
    "amino_acid = ['A', 'V', 'F', 'I', 'L','D','E','K','S','T','Y','C','N','Q', 'P','M', 'R', 'H', 'W', 'G','X'] # X is the undetermined amino acid, so total length is 21\n",
    "print('Number of unique amino acids are', np.shape(np.unique(amino_acid))[0])\n",
    "\n",
    "def onehotseq(sequence):\n",
    "  seq_len = np.shape(sequence)[0]\n",
    "  # print('Seq len is', seq_len)\n",
    "  seq_en = np.zeros(( seq_len, np.shape(amino_acid)[0]))\n",
    "  for i in range(seq_len):\n",
    "    if sequence[i] in amino_acid:\n",
    "      pos = amino_acid.index(sequence[i])\n",
    "      seq_en[i,pos] = 1\n",
    "    else:\n",
    "      pos = amino_acid.index('X')\n",
    "      seq_en[i,pos] = 1\n",
    "\n",
    "  return seq_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input sequence data\n",
    "def one_hot_encoding(input_sequence, num_files, seq_length):\n",
    "    ohe = np.zeros((input_sequence.shape[0], input_sequence.shape[1],input_sequence.shape[2], len(amino_acid)))\n",
    "\n",
    "    for i in range(input_sequence.shape[0]):\n",
    "        for j in range(input_sequence.shape[1]):\n",
    "            seq_len = int(seq_length[i,j])\n",
    "            if seq_len > 0:\n",
    "                seq_en = onehotseq(input_sequence[i,j,0:seq_len])\n",
    "                ohe[i,j,0:seq_len,:] = seq_en\n",
    "    return ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (bnn1): Linear(in_features=21, out_features=32, bias=True)\n",
       "  (bnn2): Linear(in_features=32, out_features=64, bias=True)\n",
       "  (bnn3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (bnn4): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (lstm1): LSTM(64, 512, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (bn_lstm1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (nn1): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "  (nn2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (nn3): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (nn4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (nn5): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (nn6): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (nn7): Linear(in_features=128, out_features=32, bias=True)\n",
       "  (nn8): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (tanh): Tanh()\n",
       "  (drop): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# LSTM Model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, num_layers, seq_len,num_classes=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size1 = hidden_size1\n",
    "        self.hidden_size2 = hidden_size2\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.bnn1 = nn.Linear(input_size, 32)\n",
    "        self.bnn2 = nn.Linear(32,64)\n",
    "        self.bnn3 = nn.Linear(64,64)\n",
    "        self.bnn4 = nn.Linear(64,64)\n",
    "\n",
    "        self.lstm1 = nn.LSTM(64, hidden_size1, num_layers, batch_first=True, bidirectional=True, dropout=0.5)\n",
    "        self.bn_lstm1 = nn.BatchNorm1d(2*hidden_size1,device=device)  \n",
    "        # self.lstm2 = nn.LSTM(2*hidden_size1, hidden_size2, num_layers, batch_first=True, bidirectional=True, dropout=0.5)\n",
    "        # self.bn_lstm2 = nn.BatchNorm1d(2*hidden_size2,device=device)\n",
    "        self.nn1 = nn.Linear(2*hidden_size1, 2*hidden_size1)\n",
    "        self.nn2 = nn.Linear(2*hidden_size1, 512)\n",
    "        self.nn3 = nn.Linear(512, 512)\n",
    "        self.nn4 = nn.Linear(512, 256)\n",
    "        self.nn5 = nn.Linear(256, 256)\n",
    "        self.nn6 = nn.Linear(256, 128)\n",
    "        self.nn7 = nn.Linear(128, 32)\n",
    "        self.nn8 = nn.Linear(32, 1)\n",
    "\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "        # self.batch = nn.BatchNorm1d()\n",
    "        self.drop = nn.Dropout(p=0.5)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x, array_lengths):\n",
    "        # Set initial hidden states (and cell states for LSTM)\n",
    "        # print(x.size(0))\n",
    "        inital_seq_len = x.size(1)\n",
    "        x = Variable(x.float()).to(device)\n",
    "\n",
    "        x = torch.reshape(x, (x.size(0)*x.size(1), x.size(2)))\n",
    "\n",
    "        ## before nn\n",
    "        out = self.bnn1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.bnn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.bnn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.bnn4(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        ## reshaping again\n",
    "        out = torch.reshape(out, (-1, inital_seq_len, out.size(1)))\n",
    "        # print(out.size())\n",
    "        # print(aaaaa)\n",
    "\n",
    "        # out = torch.permute(out, (0,2,1))\n",
    "        \n",
    "        pack = nn.utils.rnn.pack_padded_sequence(out, array_lengths, batch_first=True, enforce_sorted=False)\n",
    "        h0 = Variable(torch.zeros(2*self.num_layers, x.size(0), self.hidden_size1).to(device))\n",
    "        c0 = Variable(torch.zeros(2*self.num_layers, x.size(0), self.hidden_size1).to(device))\n",
    "        h1 = Variable(torch.zeros(2*self.num_layers, self.hidden_size1, self.hidden_size2).to(device))\n",
    "        c1 = Variable(torch.zeros(2*self.num_layers, self.hidden_size1, self.hidden_size2).to(device))\n",
    "        \n",
    "        # Forward propagate RNN\n",
    "        out, _ = self.lstm1(pack, (h0,c0))\n",
    "        del(h0)\n",
    "        del(c0)\n",
    "        # out, _ = self.lstm2(out, (h1,c1))\n",
    "        gc.collect()\n",
    "        unpacked, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True)\n",
    "        this_batch_len = unpacked.size(1)\n",
    "        out = unpacked\n",
    "        # print('before', out.size())\n",
    "        out = torch.reshape(out, (out.size(0)*out.size(1), out.size(2)))\n",
    "\n",
    "        ##nn\n",
    "        out = self.nn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.nn2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.nn3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.nn4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.nn5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.nn6(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.nn7(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.nn8(out)\n",
    "        \n",
    "        ## reshaping\n",
    "        out = torch.reshape(out, (-1, this_batch_len, 1))\n",
    "        # print(out.size()) \n",
    "        # print(aaaaa)   \n",
    "        \n",
    "\n",
    "        return out\n",
    "\n",
    "model_test = torch.load('/home/apa2237/spider/B_Factor_Model/epoch_177.pth', map_location='cuda')\n",
    "model_test.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b_factor_cal(ohe, num_files, seq_length):\n",
    "  \n",
    "  b_factor = np.zeros((ohe.shape[0], ohe.shape[1], ohe.shape[2]))\n",
    "  ohe = torch.from_numpy(ohe).to(device)\n",
    "  # max_seq_len = int(np.max(seq_length))\n",
    "  # print(max_len)\n",
    "  with torch.no_grad():\n",
    "    for i in range(ohe.shape[0]):\n",
    "      if int(num_files[i,]) !=0:\n",
    "        input_x = ohe[i,0:int(num_files[i,]),:,:]        \n",
    "        input_x = torch.reshape(input_x, (input_x.shape[0],max_len,len(amino_acid)))  \n",
    "        # print(input_x.shape)\n",
    "        array_lengths = torch.from_numpy(seq_length[i,0:int(num_files[i,])])\n",
    "        outputs = model_test(input_x, array_lengths)\n",
    "        outputs = torch.reshape(outputs, (outputs.shape[0], outputs.shape[1]))\n",
    "        b_factor[i,0:int(num_files[i,]),0:int(torch.max(array_lengths))] = outputs.to('cpu').numpy()\n",
    "  \n",
    "  return b_factor\n",
    "\n",
    "# ohe = ohe.to('cpu').numpy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making heat map for RC 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rc_cal(b_factor, ohe, input_sequence, num_files, seq_length):\n",
    "\n",
    "    filter = 7 ##THESE ARE VARIABLES. YOU CAN CHANGE THESE TO SUIT YOUR REQUIREMENTS. THIS DEFINES THE MAX(M) VALUE IN THE PAPER. MAX(M)=FILTER-1\n",
    "    stride = 1\n",
    "    rc = filter-1\n",
    "    heat_map = np.zeros((input_sequence.shape[0], len(amino_acid), len(amino_acid), rc, 2))\n",
    "\n",
    "\n",
    "    for prot in range(heat_map.shape[0]):\n",
    "        for fl in range(int(num_files[prot,])):\n",
    "            # print(num_files[prot,])\n",
    "            \n",
    "            subs_map = np.zeros((len(amino_acid), len(amino_acid), rc,2))\n",
    "            index = 0\n",
    "            conti = True \n",
    "            b_mean = np.mean(b_factor[prot,fl,0:int(seq_length[prot,fl])])\n",
    "            b_std =  np.std(b_factor[prot,fl,0:int(seq_length[prot,fl])])        \n",
    "                \n",
    "            while conti:\n",
    "                conv_unit = np.argmax(ohe[prot,fl,index:index+filter, :], axis=1)\n",
    "                b_unit = b_factor[prot,fl,index:index+filter]\n",
    "                look = np.arange(1,len(b_unit))\n",
    "                \n",
    "                for i in look:\n",
    "                    subs_map[conv_unit[0], conv_unit[i],i-1,0] += (( b_unit[0] - b_mean)/b_std)\n",
    "                    subs_map[conv_unit[0], conv_unit[i],i-1,1] += (( b_unit[i] - b_mean)/b_std)\n",
    "                    \n",
    "                index += stride\n",
    "                conti = ((index + filter) < seq_length[prot,fl])\n",
    "            \n",
    "            for m in range(rc):\n",
    "                heat_map[prot,:,:,m,:] += (subs_map[:,:,m,:]/(seq_length[prot,fl]-(m+1)))\n",
    "        \n",
    "        if num_files[prot,] != 0:   \n",
    "            heat_map[prot,:,:,:,:] =  heat_map[prot,:,:,:,:]/num_files[prot,]\n",
    "\n",
    "    heat_map = heat_map[:,0:len(amino_acid)-1,0:len(amino_acid)-1,:,:]       \n",
    "    \n",
    "    return heat_map\n",
    "    # np.save('/home/apa2237/spider/heatmap_codes/variables/b_factor_rc_3', heat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_cals(heat_map, output_properties, id, num_files):\n",
    "    corr = np.zeros((heat_map.shape[1], heat_map.shape[2], heat_map.shape[3], heat_map.shape[4]))\n",
    "\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(corr.shape[1]):\n",
    "            for k in range(corr.shape[2]):\n",
    "                for l in range(corr.shape[3]):\n",
    "                    index = ((np.isnan(output_properties[:,id])==False) & (num_files != 0))\n",
    "                    pcc_prop = output_properties[index,id].reshape((1,-1))\n",
    "                    pcc_fea = heat_map[index,i,j,k,l].reshape((1,-1))\n",
    "                    if np.isnan(np.corrcoef(pcc_prop,pcc_fea)[0,1]):\n",
    "                        corr[i,j,k,l] = 0\n",
    "                    else:\n",
    "                        corr[i,j,k,l] = np.corrcoef(pcc_prop,pcc_fea)[0,1]\n",
    "    \n",
    "    return corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prop_cal(present, prop):\n",
    "    output_properties = np.zeros((len(present), 12))\n",
    "    count = 0\n",
    "    for ele in present:\n",
    "        pos = int(np.where(prop[:,0]==ele)[0])\n",
    "        output_properties[count,:] = prop[pos,1:13]\n",
    "        count += 1\n",
    "    return output_properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_eng_cals(heat_map,corr, lim, total_prot, current):\n",
    "    \n",
    "    imp_ones = ['MaSp1', 'MaSp2']\n",
    "    good_features = 10000\n",
    "    # good_features = np.sum(corr>lim) + np.sum(corr<-lim)\n",
    "    feature_engineered = np.zeros((heat_map.shape[0], good_features))\n",
    "    corr_filtered = np.zeros((good_features,))\n",
    "    feature_name = np.zeros((good_features,), dtype=object)\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    if (np.max(corr) > 0.4) or (np.min(corr) <-0.4):\n",
    "        lim = 0.3 ##THESE ARE VARIABLES. YOU CAN CHANGE THESE TO SUIT YOUR REQUIREMENTS. \n",
    "    if (np.max(corr) > 0.5) or (np.min(corr) <-0.5):\n",
    "        lim = 0.4 ##THESE ARE VARIABLES. YOU CAN CHANGE THESE TO SUIT YOUR REQUIREMENTS. \n",
    "    \n",
    "    if current in imp_ones:\n",
    "        lim = 0.18 ##THESE ARE VARIABLES. YOU CAN CHANGE THESE TO SUIT YOUR REQUIREMENTS. \n",
    "\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(corr.shape[1]):\n",
    "            for k in range(corr.shape[2]):\n",
    "                for l in range(corr.shape[3]):\n",
    "                    # cond1 = ((corr[i,j,k,0] > lim) or (corr[i,j,k,1] > lim))\n",
    "                    if ((corr[i,j,k,l] > lim) or (corr[i,j,k,l] < -lim))  & (np.sum(heat_map[:,i,j,k,l]!=0)>=0.1*total_prot):\n",
    "                        feature_engineered[:,count] = heat_map[:,i,j,k,l]\n",
    "                        corr_filtered[count,] = corr[i,j,k,l]\n",
    "                        feature_name[count,] = current+ '_' +str(amino_acid[i])+ '_'  + str(amino_acid[j]) + '_'+ str(k+1)\n",
    "                        count += 1\n",
    "                        # print(heat_map[:,i,j,k])\n",
    "                    \n",
    "    print('Number of features:', count, np.max(corr_filtered[0:count,]), np.min(corr_filtered[0:count,]))\n",
    "    \n",
    "    return feature_engineered[:,0:count],feature_name[0:count, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/apa2237/miniconda3/envs/torch/lib/python3.8/site-packages/numpy/lib/function_base.py:2853: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/home/apa2237/miniconda3/envs/torch/lib/python3.8/site-packages/numpy/lib/function_base.py:2854: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 133 0.3250049103951102 -0.3098703722786395\n",
      "Number of features: 130 0.34757885871402905 -0.28778101523432376\n",
      "Number of features: 408 0.6730455280751725 -0.6769008618553843\n",
      "Number of features: 162 0.30183501759808384 -0.35775998467589665\n",
      "Number of features: 51 0.32758120126032436 -0.26537791931577637\n",
      "Number of features: 27 0.41484790587666887 -0.37681411808448434\n",
      "Total features: 911\n"
     ]
    }
   ],
   "source": [
    "prop_id = 4\n",
    "lim = 0.2\n",
    "all_features =  np.zeros((all_present.shape[0], 10000))\n",
    "all_names = np.zeros((10000,), dtype=object)\n",
    "count = 0\n",
    "\n",
    "for i in range(len(accepted_type)):\n",
    "    spidroin = [accepted_type[i]]\n",
    "    input_sequence, num_files, seq_length = preparing_data(all_present, spidroin)\n",
    "    ohe = one_hot_encoding(input_sequence, num_files, seq_length)\n",
    "    # print(ohe.shape)\n",
    "    b_factor = b_factor_cal(ohe, num_files, seq_length)\n",
    "    heat_map = rc_cal(b_factor, ohe, input_sequence, num_files, seq_length)\n",
    "    output_properties = prop_cal(all_present, prop)\n",
    "    corr = corr_cals(heat_map, output_properties, prop_id, num_files)\n",
    "    \n",
    "    \n",
    "    for s in range(corr.shape[0]):\n",
    "        for t in range(corr.shape[1]):\n",
    "            for k in range(corr.shape[2]):    \n",
    "                name_feature[accepted_type[i]].append(amino_acid[s] + amino_acid[t]+str(k+1))\n",
    "                corr_feature[accepted_type[i]].append(corr[s,t,k])\n",
    "            \n",
    "            \n",
    "    \n",
    "    features, names  = feature_eng_cals(heat_map,corr, lim, np.sum(num_files!=0), accepted_type[i])\n",
    "    size = features.shape[1]\n",
    "    all_features[:,count:count+size] = features\n",
    "    all_names[count:count+size,] = names\n",
    "    count += size\n",
    "\n",
    "print('Total features:', count)\n",
    "np.save('./tensile_features', all_features[:, 0:count])\n",
    "np.save('./tensile_output', output_properties[:,prop_id])\n",
    "np.save('./tensile_features_name', all_names[0:count,])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "98604b3b45ec6a06ff44cc545e23221fcb3349f85795d5ab7ba8ebf01f1c77b6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
